# dashboard
import streamlit as st
from streamlit_elements import dashboard
from streamlit_elements import nivo, elements, mui, media

# data 
import pandas as pd
import numpy as np
import re
import time
import datetime
import calendar

from tqdm import tqdm
from stqdm import stqdm

from lifetimes.plotting import *
from lifetimes.utils import *
from lifetimes.plotting import *
from lifetimes.utils import *
from lifetimes import BetaGeoFitter
from lifetimes.fitters.gamma_gamma_fitter import GammaGammaFitter

from hyperopt import hp, fmin, tpe, rand, SparkTrials, STATUS_OK, space_eval, Trials




# scraping
from webdriver_manager.chrome import ChromeDriverManager
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

# google cloud platform
import io
from googleapiclient.discovery import build
from google.cloud import storage, bigquery
from google.oauth2 import service_account

# ì¼ë¶€ css
with open( "webtoon.css" ) as css:
    st.markdown( f'<style>{css.read()}</style>' , unsafe_allow_html= True)
pd.set_option('mode.chained_assignment',  None)

# ë°ì´í„° ìˆ˜ì§‘ ë‚ ì§œ
now = datetime.datetime.now()
now_time = now.strftime("%Y-%m-%d")
st.caption(now_time) #.strftime('%Y-%m-%d %H:%M'))


# ì›¹ë¸Œë¼ìš°ì €ë¥¼ ì—´ì§€ ì•Šê³  í¬ë¡¤ë§ í•˜ë ¤ë©´ headless ì˜µì…˜ì„ ì£¼ë©´ ëœë‹¤.

# chrome_options = Options()
# chrome_options.add_argument('--headless')  # ì›¹ ë¸Œë¼ìš°ì €ë¥¼ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰í•  ê²½ìš° ì¶”ê°€
# driver = webdriver.Chrome(options=chrome_options) # options=chrome_options

# ì—í”¼ì†Œë“œë³„ ëŒ“ê¸€ ì •ë³´ (user_nick, comment_date)
def get_comment_by_ep(start,end):
    result_list = []

    for i in stqdm(range(start, end + 1)):

        driver.get(f"https://comic.naver.com/webtoon/detail?titleId=811721&no={i}")
        time.sleep(0.2)
        
        episode_title = driver.find_element(By.XPATH,'//*[@id="subTitle_toolbar"]').text
        time.sleep(0.2)

        total_comment = driver.find_element(By.XPATH, '//*[@id="cbox_module_wai_u_cbox_sort_option_tab2"]') # '//*[@id="cbox_module_wai_u_cbox_sort_option_tab2"]'
        total_comment.click()
        time.sleep(0.2)
                                                        
        while True:
            try:
                # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
                more_btn = driver.find_element(By.CLASS_NAME, 'u_cbox_btn_more')
                more_btn.click()

                # ìƒˆë¡œìš´ ëŒ“ê¸€ ë¡œë”©ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì‹œê°„ì„ ì¡°ì ˆí•˜ì…”ì„œ ì ì ˆíˆ ëŒ€ê¸°ì‹œê°„ì„ ì„¤ì •í•˜ì„¸ìš”)
                time.sleep(1)

                # ì—…ë°ì´íŠ¸ëœ ëŒ“ê¸€ ìš”ì†Œë“¤ì„ ë‹¤ì‹œ ì°¾ê¸°
                user_ids = driver.find_elements(By.CLASS_NAME, 'u_cbox_name_area')
                comment_dates = driver.find_elements(By.CLASS_NAME, 'u_cbox_date')
                comment_likes = driver.find_elements(By.CLASS_NAME, 'u_cbox_cnt_recomm')
                comment_dislikes = driver.find_elements(By.CLASS_NAME,'u_cbox_cnt_unrecomm')
            except Exception as e:
                # ë” ì´ìƒ ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒí•˜ê³  ë°˜ë³µë¬¸ íƒˆì¶œ
                break


        # ìœ ì € ì•„ì´ë””ëŠ” ë‚¨ì§€ë§Œ í´ë¦°ë´‡ì— ì˜í•´ ëŒ“ê¸€ì´ ì‚­ì œë˜ëŠ” ê²½ìš°ê°€ ìˆìŒ. 
        comment_data = {
            'episode': episode_title,
            'user_id': [],
            'comment_date': [],
            'comment_like': [],
            'comment_dislike': []
        }

        for user_id, comment_date, comment_like, comment_dislike in zip(user_ids ,comment_dates, comment_likes, comment_dislikes):
            try:
                # comment_like, comment_dislikeê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ int()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜
                comment_data['user_id'].append(user_id.text)
                comment_data['comment_date'].append(comment_date.text)
                comment_data['comment_like'].append(int(comment_like.text))
                comment_data['comment_dislike'].append(int(comment_dislike.text))

            except ValueError:
                # í•´ë‹¹ ëŒ“ê¸€ì€ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ ëŒ“ê¸€ë¡œ ì§„í–‰
                pass

        df = pd.DataFrame(comment_data)
        result_list.append(df)

    # ê° ì—í”¼ì†Œë“œë³„ ë°ì´í„°í”„ë ˆì„ì„ í•©ì¹˜ê¸°
    result_df = pd.concat(result_list, ignore_index=True)

    return result_df




# ì—…ë¡œë“œ ë‚ ì§œ ë°ì´í„°
def get_webtoon_upload_at():
    try:
        result_list = []  

        # í˜ì´ì§€ê°€ ëª‡ê°œ ìˆëŠ”ì§€ ê°€ì ¸ì˜¤ê¸°
        driver.get("https://comic.naver.com/webtoon/list?titleId=811721")
        time.sleep(0.5) 

        page_numbers = driver.find_elements(By.CLASS_NAME, 'Paginate__page--iRmGj')
        total_pages = len(page_numbers)

        for i in stqdm(range(1, total_pages + 1)):
            # ê° í˜ì´ì§€ì— ì–¼ë§ˆë‚˜ ì‘í’ˆì´ ìˆëŠ”ì§€ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
            driver.get(f"https://comic.naver.com/webtoon/list?titleId=811721&page={i}")
            time.sleep(0.8) 

            elements = driver.find_elements(By.CLASS_NAME, 'EpisodeListList__item--M8zq4')
            episode_num = len(elements)  # ì‘í’ˆì˜ ê°œìˆ˜

            for j in range(1, episode_num + 1):  
                title  = driver.find_element(By.CLASS_NAME ,'EpisodeListInfo__title--mYLjC').text
                episode = driver.find_element(By.XPATH, f'//*[@id="content"]/div[3]/ul/li[{j}]/a/div[2]/p/span').text
                upload_at = driver.find_element(By.XPATH, f'//*[@id="content"]/div[3]/ul/li[{j}]/a/div[2]/div/span[2]').text
    
                # ê° ì‘í’ˆì˜ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                result_list.append({
                    'title': title,
                    'episode': episode,
                    'upload_at': upload_at
                })

        result = pd.DataFrame(result_list)
        ep_len = len(result)
        return result, ep_len

    except Exception as e:
        print(f"error {str(e)} : ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return None


# ê° ì—í”¼ì†Œë“œë³„ ì°¸ì—¬ë„ ì§€í‘œ
def get_webtoon_info(ep_len):
    try:
        result_list = []

        for i in stqdm(range(1, ep_len+1)):
            driver.get(f"https://comic.naver.com/webtoon/detail?titleId=811721&no={i}")
            time.sleep(0.2)

            episode = driver.find_element(By.XPATH, '//*[@id="subTitle_toolbar"]').text
            time.sleep(0.2)

            like_count = driver.find_element(By.XPATH, '//*[@id="viewerView"]/div/div/div[1]/div[1]/div/div/a/em[2]').text        
            time.sleep(0.2)

            comment_count = driver.find_element(By.XPATH, '//*[@id="cbox_module"]/div/div[1]/span').text
            time.sleep(0.2)

            score = driver.find_element(By.XPATH, '//*[@id="viewerView"]/div/div/div[1]/div[1]/button[2]/span[1]/span').text
            time.sleep(0.2)

            score_count = driver.find_element(By.XPATH, '//*[@id="viewerView"]/div/div/div[1]/div[1]/button[2]/span[2]').text
            score_count = re.sub(r"\D", "", score_count)

            tag = driver.find_element(By.CLASS_NAME,'TagGroup__tag--xu0OH').text


            result_list.append({
                'episode': episode,
                'Tag' : tag,
                'like_count': like_count,
                'comment_count': comment_count,
                'score_count': score_count,
                'score': score,
                'down_at' : now_time
                })


        return  pd.DataFrame(result_list)
 

    except Exception as e:
        print(f"error {str(e)} : ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return None


# def get_webtoon_info():
#     driver.get('https://comic.naver.com/webtoon/list?titleId=811721')
#     time.sleep(0.5)

#     total_len = driver.find_element(By.XPATH, '//*[@id="content"]/div[3]/ul/li[1]/a/div[2]/p/span').text
#     total_len = re.sub(r"\D", "", total_len)
#     total_len = int(total_len)


#     cookie_len = driver.find_element(By.XPATH, '//*[@id="content"]/div[3]/div[2]/button/span[1]/strong').text  
#     cookie_len = re.sub(r"\D", "", cookie_len)
#     cookie_len = int(cookie_len)

#     ep_len = total_len - cookie_len

#     try:

#         # episode ë³„ ì§€í‘œ
#         result_list = []

#         for i in range(1, ep_len+1):
#             time.sleep(1)
#             driver.get(f"https://comic.naver.com/webtoon/detail?titleId=811721&no={i}")

#             time.sleep(2)
#             episode = driver.find_element(By.XPATH, '//*[@id="subTitle_toolbar"]').text
#             like_count = driver.find_element(By.CLASS_NAME, 'u_cnt _count').text        
#             comment_count = driver.find_element(By.CLASS_NAME, 'u_cbox_count').text
#             score = driver.find_element(By.XPATH, '//*[@id="viewerView"]/div/div/div[1]/div[1]/button[2]/span[1]/span').text
#             score_count = driver.find_element(By.XPATH, '//*[@id="viewerView"]/div/div/div[1]/div[1]/button[2]/span[2]').text
#             score_count = re.sub(r"\D", "", score_count)

#             tag = driver.find_element(By.CLASS_NAME,'TagGroup__tag--xu0OH').text



#             result_list.append({
#                 'episode': episode,
#                 'Tag' : tag,
#                 'like_count': like_count,
#                 'comment_count': comment_count,
#                 'score_count': score_count,
#                 'score': score,
#                 'down_at' : now
#                 })


#         return  pd.DataFrame(result_list)
 

#     except Exception as e:
#         print(f"error {str(e)} : ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
#         return None


def get_data ():
 
    df, ep_len = get_webtoon_upload_at()
    indicator = get_webtoon_info(ep_len)    

    # ê²°ê³¼ ì¶œë ¥
    stick_df = pd.merge(df, indicator, on='episode',how='inner')
    return stick_df



# ---------------------------------------------------------------- GOOGLE Cloud Storage ì— ë°ì´í„° ì—°ê²° ----------------------------------------------------------- #


#------------- Create API client ----------------------------------- #

credentials = service_account.Credentials.from_service_account_info(
    st.secrets["gcp_service_account"]
)
client = bigquery.Client(credentials=credentials)



#------------- storage ì— ìˆëŠ” userid ë°ì´í„° ê°€ì ¸ì˜¤ê¸° --------------- #
main_bucket = 'naver_webtoon'
data_folder = ['baeksoon/user_id/', 'baeksoon/main_data/']



def load_data(data_folder):
    client = storage.Client(credentials=credentials)
    bucket = client.bucket(main_bucket)
    blobs = bucket.list_blobs()

    # í´ë” ì•ˆì— ìˆëŠ” CSV íŒŒì¼ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.

    comment_data =  [blob for blob in blobs if blob.name.startswith(data_folder) and blob.name.endswith('.csv')]

    # CSV íŒŒì¼ì„ DataFramesë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    dfs = []
    for blob in comment_data:
        csv_data = blob.download_as_string()
        df = pd.read_csv(io.StringIO(csv_data.decode('utf-8')))
        dfs.append(df)

    # ëª¨ë“  DataFramesë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
    data = pd.concat(dfs)
    return data






# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° side_bar
with st.sidebar:
    # st.image('https://image-comic.pstatic.net/webtoon/811721/thumbnail/thumbnail_IMAG21_9a2a959a-666b-4156-8e4f-db64dfe319c6.jpg',width=200)
    with st.form(key ='searchform'):
        col1,col2= st.columns([2,2]) 
        with col1:         
            st.subheader("webtoon dataset")
        
        with col2:    
            submit_search = st.form_submit_button('GCS DATA')
            scraping = st.form_submit_button('ëŒ“ê¸€ ë°ì´í„° ìˆ˜ì§‘')

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
if submit_search:
    comment_data = load_data(data_folder[0])
    main_data = load_data(data_folder[1])

    st.session_state.comment_data = comment_data
    st.session_state.main_data = main_data


# ëŒ“ê¸€ ë°ì´í„° ìˆ˜ì§‘.

start = 71
end = 78

if scraping:
    data = get_comment_by_ep(start,end)
    st.session_state.data = data

if hasattr(st.session_state, 'data'):
   data = st.session_state.data 


if st.button('Download to CSV'):
    # íŒŒì¼ ê²½ë¡œ ë° íŒŒì¼ëª… ì„¤ì •
    file_path = f'C:\webtoon\comment_data(ep{start}~{end}).csv'  
    data.to_csv(file_path, index=False,encoding='utf-8-sig')
    st.success("Success")




st.subheader(''' 
            ğŸ–¥ï¸ ë¶„ì„í•´ ë³¼ë§Œí•œ ê³¼ì œë“¤
            ''')
st.write(''' 
        ##### â‘  íŠ¹ì • ì—í”¼ì†Œë“œì˜ íŠ¸ë˜í”½ ì¦ê°€ ì›ì¸ ë¶„ì„        
        * Dau, wau, mauì˜ ë³€ë™ì„±ì— ì˜í–¥ì„ ì¤€ ì‹œì ì´ ìˆì„ê¹Œ? ê·¸ë¦¬ê³  ì˜í–¥ì„ ì¤€ íŠ¹ì • ì—í”¼ì†Œë“œê°€ ìˆë‹¤ë©´? 
        * ì›”ë³„ StickinessëŠ” ì°¨íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ì. ë…ìë“¤ì˜ ê³ ì°©ë„ê°€ ì¦ê°€í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë§Œì•½ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ê°€ ë³´ì¸ë‹¤ë©´ ì–¸ì œë¶€í„° ì˜€ì„ê¹Œ?
        * ì›¹íˆ°ì´ ì‹œì¦Œë§ˆë‹¤ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ë¶„ì„í•˜ì—¬, ê° ì‹œì¦Œì˜ íŠ¹ì§•ê³¼ ë…ìë“¤ì˜ ì°¸ì—¬ë„ì˜ ë³€ë™ì„ ë¶„ì„í•´ë³´ì.

        ##### â‘¡ ê°™ì€ ì¥ë¥´ê°„ ì›¹íˆ° ë¹„êµ
        * ê°™ì€ ì¥ë¥´ì˜ ë‹¤ë¥¸ ì‘í’ˆë“¤ê³¼ ë¹„êµ í•˜ê³  ì‹¶ì€ë°, ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ëŒ“ê¸€ ë°ì´í„°ë¥¼ ì‘í’ˆë³„ë¡œ ìˆ˜ì§‘í•´ì•¼í•œë‹¤. ì´ê²Œ ê½¤ ì‹œê°„ì´ ë§ì´ê±¸ë¦°ë‹¤..        

        ##### â‘¢ ëŒ“ê¸€ì„ í™œìš©í•œ ë¶„ì„
        * vip ë©¤ë²„ë¥¼ êµ¬í•´ë³´ì! (ì°¸ì—¬ë„ê°€ ê°€ì¥ ë†’ì€ ì°íŒ¬ ë…ì) Recency Frequency ë¥¼ ê¸°ì¤€ìœ¼ë¡œ.. (monetrayì˜ ê²½ìš° ê³µê°ìˆ˜+ ëŒ“ê¸€ìˆ˜ ë¡œ ëŒ€ì²´)
 
        ''' )



if hasattr(st.session_state, 'main_data'):
    main_data = st.session_state.main_data
    main_data['upload_at'] = pd.to_datetime(main_data['upload_at'], format='%y.%m.%d')
    st.subheader('episode data')
    # st.write(main_data)




if hasattr(st.session_state, 'comment_data'):
    comment_data = st.session_state.comment_data


    # ì´ë²¤íŠ¸ê°€ ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ì „ì²˜ë¦¬ ì½”ë“œë“¤ì´ ì‹¤í–‰ë˜ì§€ ì•Šê²Œ cache_resource 
    @st.cache_resource
    def preprocessing (comment_data):
        # í´ë¦°ë´‡ì— ì˜í•´ ì œê±°ëœ ëŒ“ê¸€ ì œê±°
        comment_data = comment_data.dropna(axis=0)

        # Nì¼ì „ Nì‹œê°„ ì „ ê°™ì€ í˜•íƒœì˜ ê°’ì´ ìˆë‹¤.
        # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ìë§Œ ì¶”ì¶œí•˜ê³  ë°ì´í„° ìˆ˜ì§‘ì¼ì¸ '2024-03-08' ê¸°ê°„ê³¼ ëº´ì•¼í•œë‹¤.
        def extract_numbers(value):
            return int(re.sub(r"\D", "", value))  if isinstance(value, str) else None


        down_date = '2024-03-08'
        down_date = pd.to_datetime(down_date, format='%Y-%m-%d')
        
        # 'comment_date' ì»¬ëŸ¼ì˜ ê°’ì— '~ì¼ ì „' í˜•ì‹ì¸ ê²½ìš°, ìˆ«ì ì¶”ì¶œí•˜ì—¬ 'col' ì»¬ëŸ¼ì— í• ë‹¹
        comment_data['comment_date'] = comment_data['comment_date'].apply(lambda x: extract_numbers(x) if 'ì¼ ì „' in str(x) else x)
        comment_data['comment_date'] = comment_data['comment_date'].apply(lambda x: down_date - datetime.timedelta(days=x) if isinstance(x, int) else x)    

        # comment_date ì—´ì˜ ê°’ì„ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        comment_data['comment_date'] = pd.to_datetime(comment_data['comment_date'], errors='coerce') # errors='coerce' ë¥¼í†µí•´ '2ì¼ì „' ê°™ì€ ë¬¸ìì—´ ê°’ë“¤ì€ None ê°’ìœ¼ë¡œ ë°”ë€Œê²Œ ëœë‹¤.

        # ë°ì´í„°ëŠ” 2ì›” ë§ˆì§€ë§‰ì£¼ ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê²Œ
        comment_data = comment_data.dropna(subset=['comment_date'])
        comment_data = comment_data[comment_data['comment_date'] <= '2024-03-03']

        comment_data['day_name'] = comment_data['comment_date'].dt.day_name()
        # ì¼ìë³„ í™œì„± ì‚¬ìš©ì (DAU)
        dau = comment_data.groupby([comment_data['comment_date'].dt.date,'day_name'])['user_id'].nunique().reset_index() #  'day_name'
        # ì£¼ê°„ë³„ í™œì„± ì‚¬ìš©ì (WAU)
        wau = comment_data.groupby(comment_data['comment_date'].dt.to_period('W').dt.start_time.dt.date)['user_id'].nunique().reset_index()
        # ì›”ê°„ë³„ í™œì„± ì‚¬ìš©ì (MAU)
        mau = comment_data.groupby(comment_data['comment_date'].dt.to_period('M').dt.start_time.dt.date)['user_id'].nunique().reset_index()
        return  comment_data, dau, wau, mau
    
    comment_data, dau, wau, mau = preprocessing(comment_data)

    unique_user  = len(comment_data['user_id'].unique())  # ëŒ“ê¸€ì„ ë‹´ê¸´ ìœ ë‹ˆí¬í•œ ìœ ì €

    with st.container():
        st.header(''' 
                Activation User  
                ''')
        st.caption(''' 
                   * í™œì„±í™” ìœ ì €ì˜ ê¸°ì¤€ì€ 'ëŒ“ê¸€'ì„ ë‚¨ê¸´ ìœ ì €ë¡œ ì •í–ˆì–´ìš”. ëŒ“ê¸€ê³¼ ì¢‹ì•„ìš”ëŠ” ì›¹íˆ°ì„ ë³´ê³  ë‚œ ë’¤, ì¦‰ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í–ˆë‹¤ëŠ” ê°€ì¥ í™•ì‹¤í•œ í”ì ì´ ë¼ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. 
                   * ê·¸ ì¤‘ ì—í”¼ì†Œë“œë³„ ë‚¨ê²¨ì§„ ëŒ“ê¸€ì˜ ì•„ì´ë””ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³ ìœ  ìœ ì €ìˆ˜ë¥¼ ì§‘ê³„ í–ˆì–´ìš”ğŸ«¡! ì›¹íˆ° í˜ì´ì§€ì—ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ëŒ“ê¸€ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ì¼ë¶€ í•„í„°ë§ëœ 'ë‹‰ë„¤ì„(id***)' ì´ ê°™ë‹¤ë©´ ë™ì¼ ìœ ì €ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤. 
                     
                    ''')

        

        max_date = max(comment_data['comment_date'].dt.date) 
        min_date = min(comment_data['comment_date'].dt.date)


        # ë‚ ì§œ, activation ì˜µì…˜ columns
        col1,col2 = st.columns([1,4])
        with col1:
            d = st.date_input(
                "ë‚ ì§œ",
                (min_date, max_date),
                min_date, # ìµœì†Œ ë‚ ì§œ
                max_date, # ìµœëŒ€ ë‚ ì§œ
                format="YYYY.MM.DD",
            ) 
            if len(d) >= 2: 
                start_d = d[0]
                end_d = d[1]
            else:
                start_d = d[0]
                end_d = max_date

        with col2:

            indication = st.radio(
                "ìœ ì € í™œì„±í™” ì§€í‘œ",
                ["DAU", "WAU", "MAU"],
                 horizontal=True, label_visibility="visible"
            )
        # ì„ íƒëœ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
            if indication == "DAU":
                df = dau
            elif indication == "WAU":
                df = wau
            elif indication == "MAU":
                df = mau
            





        #     st.write(f''' í˜„ì¬ {unique_user}ëª…ì˜ ë…ìê°€ ì›¹íˆ°ì„ ë³´ê³  ëŒ“ê¸€ì„ ë‚¨ê²¼ì–´ìš”. 'ê°œê·¸' ì¥ë¥´ì˜ ë‹¤ë¥¸ ì‘í’ˆì— ë¹„í•´ % ë†’ì€ ìˆ˜ì¹˜ì…ë‹ˆë‹¤!''')


        # í™œì„±í™” ì§€í‘œë³„ ì‹œê°í™” í•¨ìˆ˜
        def user_active_chart (df,title,color):
            title = indication
            date_mask = (df['comment_date'] >= start_d) & (df['comment_date'] <= end_d) # dateë¡œ ì§€ì •ëœ ê°’ë“¤ë§Œ 
            df = df.loc[date_mask]
            pivot = pd.pivot_table(df, values='user_id', index='comment_date')



            st.subheader(f'ğŸ“Š {title}')
            st.line_chart(pivot, use_container_width=True,color=color)



        col1,col2 = st.columns([3,1])
        with col1:
            
            user_active_chart(df,'ğŸ“Š Daily Active User','#75D060')
    
            st.markdown(''' 
                    > * ë¬´ì§ë°±ìˆ˜ê³„ë°±ìˆœ ì›¹íˆ°ì˜ ê²½ìš° ì¼ìš”ì¼, ìˆ˜ìš”ì¼ì— ì—°ì¬ë˜ëŠ” ì‘í’ˆì…ë‹ˆë‹¤. ì •í•´ì§„ ìš”ì¼ì—ë§Œ ì—°ì¬ë˜ëŠ” ì›¹íˆ° íŠ¹ì„±ìƒ ìš”ì¼ë³„ë¡œ í° ë³€ë™ì„±ì´ ìˆì—ˆìŠµë‹ˆë‹¤.                                                                 
                    ''')
   

        # issue
        with col2:
            # st.subheader(' âœ”ï¸Issue')

            st.markdown('''
                ##### ë…ìë“¤ì˜ ì¬ë°©ë¬¸ì´ ë†’ì„ê¹Œ?              
                         ''')
            st.caption('* í•´ë‹¹ ì£¼ì˜ ì „ì²´ DAUê°€ WAU ë³´ë‹¤ ë†’ì€ ê²½ìš° : ì¬ë°©ë¬¸í•˜ëŠ” ë…ìë“¤ì´ ë§ì€ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ')

            #--------------------------------- ì¬ë°©ë¬¸ ê·¸ë˜í”„ ---------------------------------------------- #

            dau = dau.rename(columns={'user_id':'dau'})
            dau['comment_date'] = pd.to_datetime(dau['comment_date'], errors='coerce')
            dau['week'] = dau['comment_date'].dt.to_period('W').dt.start_time.dt.date
            dau['month'] = dau['comment_date'].dt.to_period('M').dt.start_time.dt.date

            dw = pd.merge(dau, wau, left_on='week', right_on='comment_date', how='inner')
            dw = dw.rename(columns={'user_id': 'wau','comment_date':'week','comment_date_x':'day'}).drop(columns=['comment_date_y']) 
            dw['dau_sum'] = dw.groupby(['week'])['dau'].transform('sum')
            dw['dau_wau_diff'] = dw['dau_sum'] - dw['wau']
            dw= dw.drop_duplicates(subset=['week'])[['week','dau_sum','wau','dau_wau_diff']]


            # chort_df = comment_data.copy()
            # chort_df.set_index('user_id', inplace=True)
            # ìœ ì €ë³„ ì²« ì°¸ì—¬ê¸°ê°„ ì¶”ì¶œ
            # chort_df['CohortGroup'] = chort_df.groupby(level=0)['comment_date'].min().apply(lambda x: x.strftime('%Y-%m'))
            # chort_df.reset_index(inplace=True)


# ------------------------------------------------------ ëŒ“ê¸€ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœ ì € ê³ ì°©ë„ë¥¼ êµ¬í•´ë³´ì --------------------------------------------- #




        st.header('Stickiness ')
        st.caption(''' í•´ë‹¹ ì›¹íˆ°ì€ ì•„ì§ ì—°ì¬ì¼ì´ 1ë…„ì´ ì•ˆëœ ì›¹íˆ°ì…ë‹ˆë‹¤. ë˜í•œ ì£¼ì— 2ë²ˆ ì—°ì¬ë˜ëŠ” ì‘í’ˆ íŠ¹ì„±ìƒ í™œì„±í™” ìœ ì € ë˜í•œ í•´ë‹¹ ì—°ì¬ì¼ì— ì£¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì—
                    wauë¥¼ ë‚˜ëˆ ì„œ êµ¬í•˜ëŠ”ê²ƒì´ ì ì ˆí•´ ë³´ì˜€ìŠµë‹ˆë‹¤.
                    ''')



        # ì£¼ê°„ stick êµ¬í•˜ê¸° 
        # ì¼ë‹¨, í•´ë‹¹ ìš”ì¼ì´ ì–´ëŠ ì£¼ì¸ì§€ filter í•„ìš”
        # í•´ë‹¹ í…Œì´ë¸”ì— wau merge   
        stick_df = pd.merge(dau, wau, left_on='week', right_on='comment_date', how='inner')
        stick_df = stick_df.rename(columns={'user_id': 'wau','comment_date':'week','comment_date_x':'day'}).drop(columns=['comment_date_y']) 
        stick_df['week_stick'] = round(stick_df['dau'] / stick_df['wau'],2) * 100
        stick_df['week_stick'] = stick_df['week_stick'].astype(int)
        


        # ì—°ì¬ë˜ëŠ” ë‚ ì§œì˜ ìœ ì € ê³ ì°©ë„ êµ¬í•˜ê¸°
        def Stickiness(stick_df ,day):
            Stickiness = stick_df[stick_df['day_name'].isin(day)]
            Stickiness['week'] = pd.to_datetime(Stickiness['week']).dt.strftime('%Y-%m-%d')
            Stickiness = Stickiness.groupby(['week']).agg(
                week_stick_mean = pd.NamedAgg(column='week_stick', aggfunc='mean')                                                   
                                                    ).reset_index()
            Stickiness['week_stick_mean']=round(Stickiness['week_stick_mean'])

            # (ì—°ì¬ë˜ëŠ” ë‚ ì˜)í‰ê·  ê³ ì°©ë„
            mean_stick = round(Stickiness['week_stick_mean'].mean())

            # ë°ì´í„° ë³€í™˜
            nivo_data = []
            for index, row in Stickiness.iterrows():
                nivo_data.append({'x': row['week'], 'y': row['week_stick_mean']})

            nivo_data = [{
                "id": "stickness",
                "data": nivo_data
            }]
            return mean_stick, nivo_data



        col1, col2 = st.columns([3,1])
        with col1:
            st.markdown('''#### ğŸ“Š Stickness ''' )
            mean_stick, nivo_data =  Stickiness(stick_df, day = list(calendar.day_name))

            on = st.toggle('(ì—°ì¬ì¼) Stickness')
            if on:
                mean_stick, nivo_data =  Stickiness(stick_df, day = ['Sunday','Wednesday'])




            with st.container():       
                    with elements("playlist_line_chart"):
                        layout = [
                            dashboard.Item("item_1", 0, 0, 12, 2),
                        ]

                        with dashboard.Grid(layout):                                                            
                            mui.Box(                                        
                                nivo.Line(
                                    data= nivo_data,
                                    margin={'top': 40, 'right': 30, 'bottom': 30, 'left': 30},
                                    # xScale={'type': 'point',
                                    #         },

                                    curve="monotoneX",
                                    axisTop=None,
                                    axisRight=None,
                                    axisBottom={
                                        'format': '%y-%m-%d',  # '%Y-%m-%d'
                                        'legendOffset': -12,
                                        'tickValues': 'every 30 days'
                                    },
                                    xFormat="time:%Y-%m-%d",
                                    xScale={
                                        'format': '%Y-%m-%d',
                                        'precision': 'day',
                                        'type': 'time',
                                        # 'useUTC': False
                                    },
                                    colors= {'scheme': 'accent'},

                                    enableGridX = False,
                                    enableGridY = False,
                                    enableArea = True,
                                    areaOpacity = 0.2,
                                    lineWidth=2,
                                    pointSize=3,
                                    pointColor='white',
                                    pointBorderWidth=0.5,
                                    pointBorderColor={'from': 'serieColor'},
                                    pointLabelYOffset=-12,
                                    useMesh=True,
                                    legends=[
                                                {
                                                'anchor': 'top-left',
                                                'direction': 'column',
                                                'justify': False,
                                                # 'translateX': -30,
                                                # 'translateY': -200,
                                                'itemsSpacing': 0,
                                                'itemDirection': 'left-to-right',
                                                'itemWidth': 80,
                                                'itemHeight': 15,
                                                'itemOpacity': 0.75,
                                                'symbolSize': 12,
                                                'symbolShape': 'circle',
                                                'symbolBorderColor': 'rgba(0, 0, 0, .5)',
                                                'effects': [
                                                        {
                                                        'on': 'hover',
                                                        'style': {
                                                            'itemBackground': 'rgba(0, 0, 0, .03)',
                                                            'itemOpacity': 1
                                                            }
                                                        }
                                                    ]
                                                }
                                            ],                            
                                    theme={
                                            # "background-color": "rgba(158, 60, 74, 0.2)",
                                            "textColor": "black",
                                            "tooltip": {
                                                "container": {
                                                    "background": "#3a3c4a",
                                                    "color": "white",
                                                }
                                            }
                                        },
                                    markers=[{                                                
                                        'axis': 'y',
                                        'legend': 'mean',
                                        'lineStyle': {
                                            'stroke': '#b0413e',
                                            'strokeWidth': 1
                                        },
                                        'value': mean_stick                                                
                                    }] ,                                             
                                    animate= False)
                                    ,key="item_1",sx={"borderRadius":"15px", "borderRadius":"15px","background-color":"#F0F2F6"}) 





        with col2:
            
            st.write(f''' 
                    #### ìš”ì¼ë³„ í‰ê·  stickiness (ë§‰ëŒ€ì°¨íŠ¸)                     
                    * ì§€ë°œë‹˜ì˜ ì‘í’ˆ 'ë¬´ì§ë°±ìˆ˜ ê³„ë°±ìˆœ'ì˜ í‰ê·  ê³ ì°©ë„(DAU/WAU)ëŠ” <strong style="color:#75D060"> {mean_stick}% </strong>ì…ë‹ˆë‹¤.  
                    * í° ë³€ë™ ì—†ì´ 7ì¼ ì¤‘ í‰ê·  <strong style="color:#75D060"> {(mean_stick/100)*7}ë²ˆ </strong> ëŒ“ê¸€ì„ ë‚¨ê¸°ê³  ìˆìŠµë‹ˆë‹¤.  
                    * í•œ ì£¼ë‹¹ 2ë²ˆ ì—°ì¬ë˜ëŠ” ì›¹íˆ° ì‹œìŠ¤í…œì„ ê³ ë ¤í•œë‹¤ë©´ ì•„ì£¼ ì¤€ìˆ˜í•œ ìƒíƒœë¼ê³  ìƒê°í•©ë‹ˆë‹¤.ğŸ˜€ 
                     ''',unsafe_allow_html=True )  

            # st.write(published_day)
            # stick_by_day = stick_df.groupby(['day_name']).agg(stickiness_mean = pd.NamedAgg(column='week_stick', aggfunc='mean')).reset_index()
            # stick_by_day['stickiness_mean'] =stick_by_day['stickiness_mean'].round(2)




        # total_stick = round(total_dau['stick'].sum()/ len(total_dau),2)
        # serialize_dau = total_dau[total_dau['day_name'].isin(['Sunday','Wednesday'])]
        # serialize_stick =round(serialize_dau['stick'].sum() / len(serialize_dau),2)

        # st.write(f''' í•´ë‹¹ í•˜ëŠ” ì›”ì˜ stickiness (ìœ ì € ê³ ì°©ë„)ë¥¼ êµ¬í•´ ë³´ì•˜ìŠµë‹ˆë‹¤!  
        #          ì „ì²´ ë‚ ì§œë¥¼ í¬í•¨í•œ stickiness ê°’ì€ {total_stick}% ì…ë‹ˆë‹¤.
        #          í•˜ì§€ë§Œ ì‘ê°€ë‹˜ì˜ ì›¹íˆ°ì´ ì—°ì¬ ë˜ëŠ” ë‚ ì§œ(ì¼ìš”ì¼, ìˆ˜ìš”ì¼)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³¸ë‹¤ë©´ {serialize_stick}% ìœ¼ë¡œ ì•½ ë‘ë°°ê°€ëŸ‰ ë†’ìŠµë‹ˆë‹¤.
        #          ''')

    


    with st.container():
        st.subheader(''' 
                    ğŸ¤” ë…ìë“¤ì´ ê°€ì¥ ë§ì´ ë³´ëŠ” ì‹œê°„ëŒ€ëŠ” ì–¸ì œì¸ê°€ìš”?
                    ''')
        st.caption(''' 
                * ì›¹íˆ°ì˜ ì¡°íšŒìˆ˜ì— ëŒ€í•œ ì •í™•í•œ ê°’ì€ ì•Œ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¡°íšŒìˆ˜ì™€ ëŒ“ê¸€ê°„ì˜ ìƒê´€ì„±ì€ ë§¤ìš° ë†’ë‹¤ê³  íŒë‹¨í•˜ì—¬ ë…ìë“¤ì˜ ì°¸ì—¬ë„ ì§€í‘œì¸ 'ëŒ“ê¸€'ì„ ë‚¨ê¸´ì‹œê°„ì„ ì´ìš©í•˜ì—¬ ê°€ì¥ ë§ì´ ë³´ëŠ” ì‹œê°„ëŒ€ë¥¼ ì§‘ê³„í•´ë³´ì•˜ìŠµë‹ˆë‹¤! 
                    ''')
        # comment_data[;]
        comment_data['hour'] = comment_data['comment_date'].dt.hour #.strftime("%Y-%m-%d %H:%M:%S")
        comment_data = comment_data.dropna(subset=['hour'])   
        comment_group_by_hour = comment_data.groupby(['hour']).agg(        
            cnt = pd.NamedAgg(column='hour',aggfunc='count'))

        st.line_chart(comment_group_by_hour, use_container_width=True)

        st.write('''
                  ì›¹íˆ° ì—…ë¡œë“œê°€ ë˜ëŠ” ì‹œê°„ëŒ€ì¸ 23ì‹œ~24ì‹œ ë°¤ë¶€í„° ìƒˆë²½ ì‹œê°„ëŒ€ì— ê°€ì¥ ë§ì€ ë…ìë“¤ì´ ì ‘ì†í•¨ì„ ë³¼ ìˆ˜ ìˆì—ˆì–´ìš”! 
                 ë˜í•œ ë‚® ì‹œê°„ëŒ€ì—ëŠ” ìƒëŒ€ì ìœ¼ë¡œ 12ì‹œì— ë§ì€ ì›¹íˆ°ì„ ë³´ëŠ” ë…ìë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤.''')



        st.divider()


    with st.container():
        
        st.subheader('ğŸ… ê°€ì¹˜ê°€ ë†’ì€ ë…ì ì„ ë³„í•˜ê¸°')
        st.markdown(''' 
                ê°€ì¹˜ê°€ ë†’ì€ ë…ìë¥¼ ì„ ë³„í•˜ê¸° ìœ„í•´ **ì¿ í‚¤(ìœ ë£Œê²°ì œ)ë¥¼ ì´ìš©ì—¬ë¶€**ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. 
                ''')
        st.caption(''' 
            ì‹¤ì œë¡œ ì¿ í‚¤ë¥¼ ê²°ì œí•œ ìœ ì €ë“¤ì˜ ì •ë³´ë¥¼ 100% ì•Œ ìˆ˜ëŠ” ì—†ì—ˆì§€ë§Œ, ëŒ“ê¸€ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì–´ëŠì •ë„ ìœ ì¶”í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë°”ë¡œ <strong style="color:#6BC55C"> 'ì›¹íˆ°ì´ ì—…ë¡œë“œëœ ë‚ ì§œ'ì™€ 'ëŒ“ê¸€ì´ ì‘ì„±ëœ ë‚ ì§œ'ë¥¼ ì´ìš©</strong>í•˜ëŠ” ê²ƒì´ì£ .  
            ë§Œì•½, '2024-03-01'ì— ì—…ë¡œë“œëœ ì‘í’ˆì´ ìˆë‹¤ë©´, ìœ ë£Œê²°ì œë¥¼ í•˜ì§€ ì•Šì€ ì‚¬ëŒì˜ ê²½ìš° ì—…ë¡œë“œëœ ë‚ ì§œ ì´í›„ì— ëŒ“ê¸€ì„ ë‚¨ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ **ì¿ í‚¤ë¥¼ ì´ìš©í•˜ì—¬ ë¯¸ë¦¬ë³´ê¸°ë¥¼ í•œ ìœ ì €ì˜ ê²½ìš°
            ì—…ë¡œë“œ ë‚ ì§œ(2024-03-01) ì´ì „ì— ì›¹íˆ°ì„ ë³´ê³  ëŒ“ê¸€ì„ ì‘ì„±**í–ˆì„ ê²ƒì…ë‹ˆë‹¤! <strong style="color:#6BC55C">  
            ì¦‰, 'ì›¹íˆ°ì´ ê²Œì‹œëœ ë‚ ì§œ' > 'ëŒ“ê¸€ì´ ì‘ì„±ëœ ë‚ ì§œ'ì¸ ê²½ìš° 'ì¿ í‚¤ë¥¼ ì‚¬ìš©í•œ ë…ì' ë¡œ íŒë‹¨</strong>í–ˆìŠµë‹ˆë‹¤. 
            ''', unsafe_allow_html=True)

        info = main_data.drop_duplicates(subset=['episode'])[['episode','upload_at']]
        ltv_df = pd.merge(comment_data, info, on='episode',how='left')
        ltv_df['cookie'] = np.where(ltv_df['comment_date'] < ltv_df['upload_at'], 1, 0)
        ltv_df['price'] = ltv_df['cookie']*12000 + ltv_df['comment_like'] + 500 


        col1,col2 = st.columns([1,2])
        with col1:
            st.markdown('''##### ì¿ í‚¤ë¥¼ ì‚¬ìš©í•œ ìœ ì €ì˜ í…Œì´ë¸”(ì¼ë¶€) ''')
            st.caption('''
                    ë…ìì˜ Price ì‚°ì¶œê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
                        * ì¿ í‚¤ 1ê°œ ì´ìš© = 12000ì›ì˜ ê°€ì¹˜  
                        * ë°›ì€ ì¢‹ì•„ìš” = ê°œë‹¹ 1ì›ì˜ ê°€ì¹˜  
                        * ëŒ“ê¸€ ì‘ì„±ìˆ˜ = ê°œë‹¹ 500ì›ì˜ ê°€ì¹˜
                        ''')

            # ì¢‹ì•„ìš”ê°€ ë§ë‹¤ëŠ” ê²ƒì€ ë…ìë“¤ì´ í•´ë‹¹ ëŒ“ê¸€ì— ê³µê°í•˜ê³ , ë™ì¡°í•  í™•ë¥ ì´ ë†’ë‹¤ëŠ” ìë£Œê°€ ìˆë‹¤. í•´ë‹¹ ì‘í’ˆì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ì§€í‘œë¼ê³  ìƒê°ë˜ì—ˆìŠµë‹ˆë‹¤.
            # ë² ìŠ¤íŠ¸ëŒ“ê¸€ì— ë˜ ë‹¤ë¥¸ ëŒ“ê¸€ì„ ë‚¨ê¸°ê¸°ë„ í•˜ê³ , ì¢‹ì•„ìš”ë¥¼ ëˆŒë¥´ê¸°ë„í•˜ëŠ” ì´ëŸ° ë…ìë“¤ì˜ ì°¸ì—¬ë„ë¥¼ ì´ëŒì–´ ë‚¼ ìˆ˜ ìˆëŠ” ì§€í‘œë¼ê³  ìƒê°í–ˆê¸° ë•Œë¬¸ì— ê¸ˆì „ì ì¸ ê°€ì¹˜ê°€ ì–´ëŠì •ë„ ìˆë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.


            current_date = ltv_df['comment_date'].max()
            ltv_df['comment_date'] = pd.to_datetime(ltv_df['comment_date']).dt.date

            metrics_df = summary_data_from_transaction_data(ltv_df
                                                    , customer_id_col = 'user_id'
                                                    , datetime_col = 'comment_date'
                                                    , monetary_value_col='price'
                                                    , observation_period_end=current_date).reset_index()
            


            st.write(metrics_df[metrics_df['user_id'].str.contains('ë†ì–´')])
            st.write(metrics_df)
            sample = ltv_df[ltv_df['user_id'].str.contains('ë†ì–´')][['user_id','comment_date','price']]
            st.write(sample)
            st.write((len(sample['comment_date'].unique())-1))




        with col2:
            st.write('gd')

# if st.button('Download to CSV'):
#     # íŒŒì¼ ê²½ë¡œ ë° íŒŒì¼ëª… ì„¤ì •
#     file_path = f'C:\webtoon\comment_data(71~77).csv'  
#     comment_data.to_csv(file_path, index=False,encoding='utf-8-sig')
#     st.success("Success")